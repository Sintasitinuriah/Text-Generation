{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149e24c0",
   "metadata": {},
   "source": [
    "#### Automatic Text Summarization\n",
    "\n",
    "**Automatic Text Summarization** adalah teknik penting dalam **Natural Language Processing (NLP)** yang menggunakan algoritma untuk merangkum teks panjang sambil tetap mempertahankan informasi utamanya. Meskipun tidak sepopuler terobosan machine learning lainnya, teknologi peringkasan teks terus mengalami perkembangan yang signifikan.\n",
    "\n",
    "Dengan mengekstraksi konsep-konsep utama dan mempertahankan makna asli, sistem ini mampu merevolusi berbagai industri seperti **perbankan**, **hukum**, dan **kesehatan**, karena membantu proses pengambilan keputusan dan pencarian informasi menjadi lebih cepat.\n",
    "\n",
    "---\n",
    "\n",
    "##### Jenis-Jenis Text Summarization\n",
    "\n",
    "Terdapat dua jenis utama teknik peringkasan teks:\n",
    "\n",
    "1. **Extractive Summarization**\n",
    "2. **Abstractive Summarization**\n",
    "\n",
    "---\n",
    "\n",
    "##### Extractive Summarization\n",
    "\n",
    "**Extractive summarization** menghasilkan ringkasan dengan **memilih dan menggabungkan bagian teks terpenting** dari teks asli. Tidak seperti manusia yang bisa menyusun ulang kalimat, model ini hanya mengekstrak kalimat atau frasa penting tanpa membuat konten baru.\n",
    "\n",
    "Tujuan metode ini adalah **mempertahankan makna teks asli** sambil mengurangi panjangnya.\n",
    "\n",
    "---\n",
    "\n",
    "##### Algoritma TextRank\n",
    "\n",
    "Salah satu algoritma yang paling banyak digunakan dalam extractive summarization adalah **TextRank**.  \n",
    "TextRank bekerja dengan **memberi peringkat pada kalimat** berdasarkan relevansi dan kepentingannya, kemudian memilih kalimat terbaik untuk diringkas.\n",
    "\n",
    "---\n",
    "\n",
    "##### Menggunakan TextRank untuk Extractive Summarization\n",
    "\n",
    "TextRank dapat diimplementasikan melalui pustaka **spaCy**, dengan bantuan ekstensi **PyTextRank**.\n",
    "\n",
    "Dengan TextRank, proses peringkasan dilakukan dengan:\n",
    "- mengekstrak frasa dan kalimat penting,\n",
    "- mengurutkan kalimat berdasarkan skor kepentingan,\n",
    "- menyusun ringkasan dari kalimat dengan skor tertinggi.\n",
    "\n",
    "Namun perlu dicatat bahwa **extractive summarization tidak menghasilkan kalimat baru**, melainkan hanya memilih bagian dari teks asli.\n",
    "\n",
    "---\n",
    "##### Prasyarat\n",
    "\n",
    "Sebelum menjalankan TextRank, pastikan Anda memiliki:\n",
    "\n",
    "- **spaCy**  \n",
    "  Pustaka Python untuk berbagai tugas NLP.\n",
    "\n",
    "- **PyTextRank**  \n",
    "  Ekstensi spaCy yang menyediakan implementasi TextRank untuk analisis teks dan summarization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c255926",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Install Library yang dibutuhkan\n",
    "!pip install spacy\n",
    "!python3 -m spacy download en_core_web_lg\n",
    "\n",
    "!pip install pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd62bcb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "example_text = \"\"\"Deep learning (also known as deep structured learning) is part of a \n",
    "broader family of machine learning methods based on artificial neural networks with \n",
    "representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, \n",
    "recurrent neural networks and convolutional neural networks have been applied to\n",
    "fields including computer vision, speech recognition, natural language processing, \n",
    "machine translation, bioinformatics, drug design, medical image analysis, material\n",
    "inspection and board game programs, where they have produced results comparable to \n",
    "and in some cases surpassing human expert performance. Artificial neural networks\n",
    "(ANNs) were inspired by information processing and distributed communication nodes\n",
    "in biological systems. ANNs have various differences from biological brains. Specifically, \n",
    "neural networks tend to be static and symbolic, while the biological brain of most living organisms\n",
    "is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\n",
    "layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, \n",
    "but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
    "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, \n",
    "which permits practical application and optimized implementation, while retaining theoretical universality \n",
    "under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely \n",
    "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, \n",
    "whence the structured part.\"\"\"\n",
    "print('Original Document Size:',len(example_text))\n",
    "doc = nlp(example_text)\n",
    "\n",
    "for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
    "    print(sent)\n",
    "    print('Summary Length:',len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4be03",
   "metadata": {},
   "source": [
    "##### Abstractive Summarization\n",
    "\n",
    "**Abstractive summarization** menghasilkan **kalimat baru sepenuhnya** untuk menyampaikan ide-ide utama dari teks asli. Berbeda dengan *extractive summarization* yang hanya memilih dan menyusun ulang kalimat dari teks sumber, metode ini **memparafrasekan informasi** secara lebih ringkas dan koheren, bahkan menggunakan **kosakata baru** yang tidak ada di teks awal.\n",
    "\n",
    "Pendekatan ini lebih menyerupai cara manusia merangkum teks, karena hasilnya benar-benar merupakan **pemahaman ulang** dari isi dokumen.\n",
    "\n",
    "---\n",
    "\n",
    "##### Perkembangan Abstractive Summarization\n",
    "\n",
    "Abstractive summarization semakin populer sejak hadirnya **model Transformer**, yang merevolusi berbagai tugas NLP.  \n",
    "\n",
    "Sebelum Transformer, peringkasan teks terutama menggunakan:\n",
    "\n",
    "- **Recurrent Neural Networks (RNNs)**\n",
    "- **LSTM dan GRU**\n",
    "- Arsitektur **Encoderâ€“Decoder** konvensional\n",
    "\n",
    "Meskipun efektif, model tersebut memiliki keterbatasan dalam memahami konteks panjang.\n",
    "\n",
    "Kemunculan **Transformer** membawa arsitektur baru yang menggunakan *self-attention*, sehingga:\n",
    "\n",
    "- mampu memahami konteks panjang lebih baik,\n",
    "- menghasilkan ringkasan lebih akurat dan natural,\n",
    "- mempercepat proses pelatihan,\n",
    "- menjadi fondasi model modern seperti **BART**, **T5**, dan **GPT**.\n",
    "\n",
    "---\n",
    "\n",
    "Dengan demikian, abstractive summarization kini menjadi pendekatan terbaik untuk menghasilkan ringkasan yang **lebih alami, koheren, dan dekat dengan kemampuan manusia**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83447a13",
   "metadata": {},
   "source": [
    "##### PEGASUS: A Transformer Model for Text Summarization\n",
    "\n",
    "**PEGASUS** adalah model berbasis **Transformer** yang dirancang khusus untuk tugas **text summarization**. Tidak seperti model lain, PEGASUS menggunakan strategi *pre-training* yang unik dengan **memasukkan teknik Masked Sentence Prediction**.\n",
    "\n",
    "Pada proses pelatihan:\n",
    "- Kalimat-kalimat penting dalam dokumen **di-mask (disembunyikan)**.\n",
    "- Model kemudian dilatih untuk **menghasilkan kembali kalimat-kalimat yang hilang tersebut**.\n",
    "\n",
    "Pendekatan ini membuat PEGASUS unggul dalam menghasilkan ringkasan yang:\n",
    "- lebih **akurat**,  \n",
    "- **koheren**,  \n",
    "- dan lebih dekat dengan hasil ringkasan manusia.\n",
    "\n",
    "Strategi ini membantu model untuk memahami struktur dokumen dan menentukan informasi kunci yang relevan untuk diringkas.\n",
    "\n",
    "---\n",
    "\n",
    "##### Menggunakan PEGASUS untuk Text Summarization\n",
    "\n",
    "Untuk menggunakan model PEGASUS, beberapa pustaka perlu diinstal terlebih dahulu:\n",
    "\n",
    "##### Prasyarat Instalasi\n",
    "\n",
    "1. **Transformers (HuggingFace)**\n",
    "   - Untuk memuat dan menjalankan model PEGASUS.\n",
    "2. **PyTorch atau TensorFlow**\n",
    "   - Sebagai backend deep learning untuk menjalankan model.\n",
    "3. **SentencePiece**\n",
    "   - Digunakan untuk tokenisasi model PEGASUS.\n",
    "4. **pip**\n",
    "   - Manajer paket untuk menginstal semua dependensi.\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ’» Instalasi dengan pip\n",
    "\n",
    "```bash\n",
    "pip install transformers\n",
    "pip i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451c7da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Pick model\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "# Load pretrained tokenizer\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "example_text = \"\"\"\n",
    "Deep learning (also known as deep structured learning) is part of a broader family of machine learning\n",
    "methods based on artificial neural networks with representation learning. \n",
    "Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as \n",
    "deep neural networks, deep belief networks, deep reinforcement learning, \n",
    "recurrent neural networks and convolutional neural networks have been applied to \n",
    "fields including computer vision, speech recognition, natural language processing,\n",
    "machine translation, bioinformatics, drug design, medical image analysis, \n",
    "material inspection and board game programs, where they have produced results \n",
    "comparable to and in some cases surpassing human expert performance. \n",
    "Artificial neural networks (ANNs) were inspired by information processing and \n",
    "distributed communication nodes in biological systems. ANNs have various differences \n",
    "from biological brains. Specifically, neural networks tend to be static and symbolic,\n",
    "while the biological brain of most living organisms is dynamic (plastic) and analogue.\n",
    "The adjective \"deep\" in deep learning refers to the use of multiple layers in the network.\n",
    "Early work showed that a linear perceptron cannot be a universal classifier, \n",
    "but that a network with a nonpolynomial activation function with one hidden layer of \n",
    "unbounded width can. Deep learning is a modern variation which is concerned with an \n",
    "unbounded number of layers of bounded size, which permits practical application and \n",
    "optimized implementation, while retaining theoretical universality under mild conditions. \n",
    "In deep learning the layers are also permitted to be heterogeneous and to deviate widely \n",
    "from biologically informed connectionist models, for the sake of efficiency, trainability \n",
    "and understandability, whence the structured part.\"\"\"\n",
    "\n",
    "print('Original Document Size:',len(example_text))\n",
    "# Define PEGASUS model\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "# Create tokens\n",
    "tokens = pegasus_tokenizer(example_text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "# Generate the summary\n",
    "encoded_summary = pegasus_model.generate(**tokens)\n",
    "\n",
    "# Decode the summarized text\n",
    "decoded_summary = pegasus_tokenizer.decode(encoded_summary[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print('Decoded Summary :',decoded_summary)\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=pegasus_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summary = summarizer(example_text, min_length=30, max_length=150)\n",
    "summary[0][\"summary_text\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
